# ============================================================
# Master Configuration for Telco Churn Prediction SaaS
# ============================================================
# This file centralizes all hyperparameters, paths, and
# settings so nothing is hardcoded in source code.

# --- Project Metadata ---
project:
  name: "Telco Customer Churn Prediction" # Human-readable project name
  version: "1.0.0" # Semantic version of this release
  description: "Advanced churn prediction with explainability, drift monitoring, and next-best-action"

# --- Data Paths ---
data:
  raw_path: "data/raw/telco_churn.csv" # Path to the raw input CSV
  processed_path: "data/processed/" # Directory for cleaned/engineered data
  test_size: 0.2 # Fraction of data reserved for testing
  validation_size: 0.15 # Fraction of training data for validation
  random_state: 42 # Seed for reproducible train/test splits

# --- Feature Engineering ---
features:
  # Numerical columns to apply scaling/transformations
  numerical_columns:
    - "tenure" # Months the customer has been with the company
    - "MonthlyCharges" # Current monthly charge amount
    - "TotalCharges" # Cumulative charges over tenure

  # Categorical columns to encode
  categorical_columns:
    - "gender" # Male / Female
    - "SeniorCitizen" # 0 or 1 flag
    - "Partner" # Yes / No
    - "Dependents" # Yes / No
    - "PhoneService" # Yes / No
    - "MultipleLines" # Yes / No / No phone service
    - "InternetService" # DSL / Fiber optic / No
    - "OnlineSecurity" # Yes / No / No internet service
    - "OnlineBackup" # Yes / No / No internet service
    - "DeviceProtection" # Yes / No / No internet service
    - "TechSupport" # Yes / No / No internet service
    - "StreamingTV" # Yes / No / No internet service
    - "StreamingMovies" # Yes / No / No internet service
    - "Contract" # Month-to-month / One year / Two year
    - "PaperlessBilling" # Yes / No
    - "PaymentMethod" # Electronic check / Mailed check / Bank transfer / Credit card

  # Rolling window sizes (in months) for time-aware aggregates
  rolling_windows: [3, 6, 12] # 3-month, 6-month, 12-month windows

  # Target column
  target: "Churn" # Binary label: Yes / No

# --- Model Training ---
model:
  # Which algorithms to train and compare
  algorithms:
    - "logistic_regression" # Baseline linear model
    - "random_forest" # Ensemble of decision trees
    - "gradient_boosting" # Sklearn GradientBoosting
    - "xgboost" # Gradient boosting (XGBoost)
    - "lightgbm" # Light Gradient Boosting Machine

  # XGBoost hyperparameters  (significantly tuned)
  xgboost_params:
    n_estimators: 800 # More rounds for better convergence
    max_depth: 5 # Shallower trees = less overfitting
    learning_rate: 0.02 # Lower LR with more rounds
    subsample: 0.75 # Row sub-sampling per tree
    colsample_bytree: 0.75 # Column sub-sampling per tree
    colsample_bylevel: 0.75 # Column sub-sampling per level
    min_child_weight: 5 # Larger value = more conservative
    gamma: 0.1 # Min loss reduction to split
    reg_alpha: 0.1 # L1 regularisation
    reg_lambda: 1.5 # L2 regularisation
    scale_pos_weight: 3.0 # Weight for minority class (churn)
    eval_metric: "aucpr" # Optimise for PR-AUC
    early_stopping_rounds: 50 # Stop if no improvement for 50 rounds
    random_state: 42

  # LightGBM hyperparameters  (significantly tuned)
  lightgbm_params:
    n_estimators: 800 # More iterations
    max_depth: 6 # Limit depth
    learning_rate: 0.02 # Lower LR
    num_leaves: 40 # More leaves than default
    min_child_samples: 30 # Min data in a leaf
    subsample: 0.75 # Bagging fraction
    colsample_bytree: 0.75 # Feature fraction
    reg_alpha: 0.1 # L1
    reg_lambda: 0.5 # L2
    is_unbalance: true # Auto-handle class imbalance
    metric: "average_precision" # Optimize for PR-AUC
    early_stopping_rounds: 50
    random_state: 42

  # Random Forest hyperparameters  (tuned)
  random_forest_params:
    n_estimators: 500 # More trees
    max_depth: 12 # Controlled depth
    min_samples_split: 8 # Slightly less aggressive
    min_samples_leaf: 4 # Small leaves allowed
    max_features: "sqrt" # Classic RF feature sampling
    class_weight: "balanced_subsample" # Per-tree balanced weights
    random_state: 42

  # Logistic Regression hyperparameters  (tuned)
  logistic_regression_params:
    C: 0.3 # More regularisation
    penalty: "l2"
    solver: "lbfgs"
    max_iter: 2000
    class_weight: "balanced"
    random_state: 42

  # Gradient Boosting hyperparameters (sklearn)
  gradient_boosting_params:
    n_estimators: 600
    max_depth: 4
    learning_rate: 0.03
    subsample: 0.75
    min_samples_leaf: 20
    max_features: "sqrt"
    random_state: 42

  # Probability calibration method
  calibration_method: "isotonic" # "sigmoid" (Platt) or "isotonic"

  # Decision threshold optimization
  threshold:
    method: "profit" # "f1", "profit", or "custom"
    retention_cost: 50 # Cost ($) of a retention offer
    avg_customer_value: 500 # Average annual customer value ($)

# --- Evaluation ---
evaluation:
  metrics:
    - "roc_auc" # Area Under ROC Curve
    - "pr_auc" # Area Under Precision-Recall Curve
    - "f1" # F1 Score (harmonic mean of P & R)
    - "precision" # Precision (positive predictive value)
    - "recall" # Recall (sensitivity / true positive rate)
    - "lift_at_10" # Lift in top 10% of predictions
    - "expected_profit" # Profit from optimal threshold
  top_k_percentiles: [5, 10, 20] # Percentiles for lift/recall@k

# --- Monitoring ---
monitoring:
  psi_threshold: 0.2 # Population Stability Index alert threshold
  drift_check_frequency: "weekly" # How often to run drift checks
  performance_decay_threshold: 0.05 # Alert if metric drops by this much

# --- API ---
api:
  host: "0.0.0.0" # Bind address for the API server
  port: 8000 # Port for the API server
  model_path: "models/best_model.joblib" # Path to the serialized best model
  log_predictions: true # Whether to log each prediction

# --- Dashboard ---
dashboard:
  port: 8501 # Port for the Streamlit dashboard
  refresh_interval: 300 # Auto-refresh interval in seconds
